{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8656986-a150-44c3-bffc-2d019f393f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import logging\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import scipy\n",
    "from imutils import face_utils\n",
    "from face_utilities import Face_utilities\n",
    "from signal_processing import Signal_processing, detect_peaks, plot_peaks, filter_ppg\n",
    "from pyPPG.datahandling import load_data\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159f753b-577d-4a58-97d2-9d7c30a866fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29649bf-8b58-4466-98cb-9d04c1fa2d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:--------------------------------------------------\n",
      "INFO:root:Reading video 1 (15:50:04)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m frame \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m ret_process \u001b[38;5;241m=\u001b[39m \u001b[43mfu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_age_gender_face_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m68\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret_process \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo face detected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Semester temp/Class - BiomedicalSignals/labs/semester_project/heart-rate-from-camera/face_utilities.py:331\u001b[0m, in \u001b[0;36mFace_utilities.no_age_gender_face_process\u001b[0;34m(self, frame, type)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:    \n\u001b[0;32m--> 331\u001b[0m     shape, rects \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_landmarks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m68\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Semester temp/Class - BiomedicalSignals/labs/semester_project/heart-rate-from-camera/face_utilities.py:234\u001b[0m, in \u001b[0;36mFace_utilities.get_landmarks\u001b[0;34m(self, frame, type)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# all face will be resized to a fix size, e.g width = 200\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m#face = imutils.resize(face, width=200)\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# face must be gray\u001b[39;00m\n\u001b[1;32m    233\u001b[0m gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[0;32m--> 234\u001b[0m rects \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mface_detection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rects)\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rects)\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Semester temp/Class - BiomedicalSignals/labs/semester_project/heart-rate-from-camera/face_utilities.py:173\u001b[0m, in \u001b[0;36mFace_utilities.face_detection\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    171\u001b[0m gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m#get all faces in the frame\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m rects \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# to get the coords from a rect, use: (x, y, w, h) = face_utils.rect_to_bb(rects[0])\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rects\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"assets/data.csv\", index_col='N')\n",
    "data['HR_predicted'] = np.NaN\n",
    "data['points_sp'] = json.dumps([])\n",
    "data['points_dn'] = json.dumps([])\n",
    "display_video = True\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    if pd.isna(row['Recording']):\n",
    "        continue\n",
    "    #if index<16:\n",
    "    #    continue\n",
    "\n",
    "    ###########################################################\n",
    "    # Extract signal from face\n",
    "    ###########################################################\n",
    "    #cap = cv2.VideoCapture(\"assets/video_3.mp4\")\n",
    "    logging.info('-'*50)\n",
    "    logging.info(f\"Reading video {index} ({row['Recording']})\")\n",
    "    \n",
    "    row['Recording'] = '17:12:06'\n",
    "    if os.path.exists(f\"assets/video/{row['Recording']}.mp4\"):\n",
    "        extension = \"mp4\"\n",
    "    elif os.path.exists(f\"assets/video/{row['Recording']}.mov\"):\n",
    "        extension = \"mov\"\n",
    "    cap = cv2.VideoCapture(f\"assets/video/{row['Recording']}.{extension}\")\n",
    "    \n",
    "    fu = Face_utilities()\n",
    "    sp = Signal_processing()\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    t = time.time()\n",
    "    \n",
    "    #for signal_processing\n",
    "    BUFFER_SIZE = 100\n",
    "    \n",
    "    fps=0 #for real time capture\n",
    "    video_fps = cap.get(cv2.CAP_PROP_FPS) # for video capture\n",
    "    #print(video_fps)\n",
    "    \n",
    "    times: list = []  # size: 1 x BUFFER_SIZE\n",
    "    data_buffer: list = []  # size: 1 x BUFFER_SIZE\n",
    "    data_buffer_all: list = []  # size: 1 x videoFrames\n",
    "    filtered_data = []  # size: 1 x BUFFER_SIZE\n",
    "    fft_of_interest = []\n",
    "    freqs_of_interest = []\n",
    "    \n",
    "    bpm = 0\n",
    "    bpm_all = []  # size: 1 x videoFrames\n",
    "    \n",
    "    \n",
    "    \n",
    "    while True:\n",
    "        # grab a frame -> face detection -> crop the face -> 68 facial landmarks -> get mask from those landmarks\n",
    "        t0 = time.time()    \n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if frame is None:\n",
    "            break\n",
    "        \n",
    "        ret_process = fu.no_age_gender_face_process(frame, \"68\")\n",
    "        \n",
    "        if ret_process is None:\n",
    "            logging.warning(\"No face detected\")\n",
    "            continue\n",
    "        \n",
    "        rects, face, shape, aligned_face, aligned_shape = ret_process\n",
    "        \n",
    "\n",
    "        if display_video:\n",
    "            (x, y, w, h) = face_utils.rect_to_bb(rects[0])\n",
    "            cv2.rectangle(frame,(x,y),(x+w,y+h),(0,0,255),2)\n",
    "\n",
    "            if(len(aligned_shape)==68):\n",
    "                cv2.rectangle(aligned_face,(aligned_shape[54][0], aligned_shape[29][1]), #draw rectangle on right and left cheeks\n",
    "                        (aligned_shape[12][0],aligned_shape[33][1]), (0,255,0), 0)\n",
    "                cv2.rectangle(aligned_face, (aligned_shape[4][0], aligned_shape[29][1]), \n",
    "                        (aligned_shape[48][0],aligned_shape[33][1]), (0,255,0), 0)\n",
    "            else:\n",
    "                cv2.rectangle(aligned_face, (aligned_shape[0][0],int((aligned_shape[4][1] + aligned_shape[2][1])/2)),\n",
    "                            (aligned_shape[1][0],aligned_shape[4][1]), (0,255,0), 0)\n",
    "                \n",
    "                cv2.rectangle(aligned_face, (aligned_shape[2][0],int((aligned_shape[4][1] + aligned_shape[2][1])/2)),\n",
    "                            (aligned_shape[3][0],aligned_shape[4][1]), (0,255,0), 0)\n",
    "            \n",
    "            for (x, y) in aligned_shape: \n",
    "                cv2.circle(aligned_face, (x, y), 1, (0, 0, 255), -1)\n",
    "            \n",
    "            \n",
    "        #for signal_processing\n",
    "        ROIs = fu.ROI_extraction(aligned_face, aligned_shape)\n",
    "        green_val = sp.extract_color(ROIs)\n",
    "        logging.debug(green_val)\n",
    "        \n",
    "        data_buffer.append(green_val)\n",
    "        data_buffer_all.append(green_val)\n",
    "        \n",
    "        times.append((1.0/video_fps)*i)\n",
    "        \n",
    "        L = len(data_buffer)\n",
    "        if L <= BUFFER_SIZE:\n",
    "            if display_video:\n",
    "                cv2.putText(frame, \"Filling up buffer...\", (30,int(frame.shape[0]*0.95)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "        else:\n",
    "            data_buffer = data_buffer[-BUFFER_SIZE:]\n",
    "            times = times[-BUFFER_SIZE:]\n",
    "            fps = float(BUFFER_SIZE) / (times[-1] - times[0])\n",
    "            \n",
    "            detrended_data = sp.signal_detrending(data_buffer)\n",
    "            interpolated_data = sp.interpolation(detrended_data, times)\n",
    "            normalized_data = sp.normalization(interpolated_data)\n",
    "            \n",
    "            fft_of_interest, freqs_of_interest = sp.fft(normalized_data, fps)\n",
    "            max_arg = np.argmax(fft_of_interest)\n",
    "            bpm = freqs_of_interest[max_arg]\n",
    "            if display_video:\n",
    "                cv2.putText(frame, \"fps: {0:.2f}\".format(fps), (30,int(frame.shape[0]*0.85)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "                cv2.putText(frame, \"HR_real: {0:.2f}\".format(row['HR']), (30,int(frame.shape[0]*0.90)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "                cv2.putText(frame, \"HR_predicted: {0:.2f}\".format(bpm), (30,int(frame.shape[0]*0.95)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "            filtered_data = sp.butter_bandpass_filter(interpolated_data, (bpm-20)/60, (bpm+20)/60, fps, order = 3)\n",
    "            \n",
    "        bpm_all.append(bpm)\n",
    "    \n",
    "        # display\n",
    "        if display_video:\n",
    "            cv2.imshow(\"frame\",frame)\n",
    "            cv2.imshow(\"face\",aligned_face)\n",
    "        i = i+1\n",
    "        logging.debug(\"time of the loop number \"+ str(i) +\" : \" + str(time.time()-t0))\n",
    "        if i % int(video_fps * 10) == 0:\n",
    "            logging.info(f\"Seconds of video processed: {i / video_fps:.1f}\")\n",
    "        \n",
    "        # waitKey to show the frame and break loop whenever 'q' is pressed\n",
    "        if display_video:\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    ###########################################################\n",
    "    # Extract fiducials from signal\n",
    "    ###########################################################\n",
    "    assert len(data_buffer) == 100\n",
    "    assert len(bpm_all) == len(data_buffer_all)\n",
    "    assert fps > 20\n",
    "    # Load into PyPPG\n",
    "    # TODO:\n",
    "    # Dear PyPPG maintainers: please document that `load_data` expects a mat file\n",
    "    # And that the mat file should contain a field `Data` with a 1d array, and a field `Fs` with the sampling rate.\n",
    "    with tempfile.NamedTemporaryFile(suffix='.mat') as temp:\n",
    "        scipy.io.savemat(temp.name, {\n",
    "            'Data': data_buffer_all,\n",
    "            'Fs': fps,\n",
    "        })\n",
    "        signal = load_data(data_path=temp.name, fs=0, start_sig=0, end_sig=-1, channel=\"Pleth\", use_tk=True, print_flag=True)\n",
    "\n",
    "    filtered = filter_ppg(signal, verbose=False)\n",
    "\n",
    "    # Plot in time domain\n",
    "    points_to_plot = 2000\n",
    "    filtered = filtered[:points_to_plot-740]\n",
    "    plt.plot(signal.v[:points_to_plot], label='Original signal')\n",
    "    plt.plot(filtered[:points_to_plot], label='Filtered signal')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # peak detection\n",
    "    signal.ppg = filtered\n",
    "    try:\n",
    "        fiducials = detect_peaks(signal)\n",
    "        plot_peaks(signal, fiducials, 'Fiducial detection')\n",
    "        assert type(fiducials) == pd.DataFrame\n",
    "        assert fiducials.shape[0] > 5\n",
    "    except Exception as e:\n",
    "        logging.error(f'Excpetion at peak identification: {e}, {traceback.format_exc()}')\n",
    "        fiducials = None\n",
    "\n",
    "    ###########################################################\n",
    "    # Analysis\n",
    "    ###########################################################\n",
    "\n",
    "    #plt.plot(data_buffer_all)\n",
    "    #plt.title(f\"Detection for recording {row['Recording']}\")\n",
    "    #plt.show()\n",
    "    \n",
    "    #plt.plot(bpm_all)\n",
    "    #plt.show()\n",
    "\n",
    "    if fiducials is not None:\n",
    "        # Take bpm by the median number of frames between two consecutive systolic peaks\n",
    "        median_bpm = int(fiducials['sp'].diff().median() * 60 / fps)\n",
    "        points_sp = (fiducials['sp']/fps).astype(float).tolist()\n",
    "        points_dn = (fiducials['dn']/fps).astype(float).tolist()\n",
    "        data.loc[index, 'HR_predicted'] = median_bpm\n",
    "    else:\n",
    "        #plot_peaks(signal, pd.DataFrame(columns=['on', 'sp', 'dn', 'dp', 'off', 'u', 'v', 'w', 'a', 'b', 'c', 'd', 'e', 'f', 'p1', 'p2']), 'Fiducial detection')\n",
    "        median_bpm = 0\n",
    "        points_sp = []\n",
    "        points_dn = []\n",
    "        # TODO: reestimate fft from filtered signal\n",
    "        data.loc[index, 'HR_predicted'] = int(np.mean(bpm_all[BUFFER_SIZE:]))\n",
    "    data.loc[index, 'points_sp'] = json.dumps(points_sp)\n",
    "    data.loc[index, 'points_dn'] = json.dumps(points_dn)\n",
    "\n",
    "    logging.info(\"End of video. Total running time: \" + str(time.time() - t))\n",
    "    logging.info(f\"We should have about {int(video_fps*30)} frames, and we have {len(bpm_all)}\")\n",
    "    logging.info(f\"Final BPM before filtering: {int(bpm_all[-1])}\")\n",
    "    logging.info(f\"Average BPM before filtering: {int(np.mean(bpm_all[BUFFER_SIZE:]))}\")\n",
    "    logging.info(f\"Median BPM after filtering: {median_bpm}\")\n",
    "    logging.info(f\"Real BPM: {row['HR']}\")\n",
    "\n",
    "    pd.DataFrame(\n",
    "        data_buffer_all,\n",
    "        index=pd.Series(np.arange(0, (1/fps)*len(data_buffer_all), 1/fps), name='time'),\n",
    "        columns=['signal'],\n",
    "    ).to_csv(f\"assets/ecg_from_video/{row['Recording']}.csv\")\n",
    "    \n",
    "    \n",
    "    #break\n",
    "    #if index >= 2:\n",
    "    #    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56f5f0b-4bda-4dae-8788-c1c22194a7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d4108b-997d-40a7-a8d8-c77d491827d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"assets/data_predicted.csv\", quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f1df02-4b02-48cd-bcd4-d760cf2e89c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "wtf = pd.read_csv(f\"assets/ecg_from_video/15:50:04.csv\")\n",
    "with tempfile.NamedTemporaryFile(suffix='.mat') as temp:\n",
    "    scipy.io.savemat(temp.name, {\n",
    "        'Data': wtf['signal'].values,\n",
    "        'Fs': wtf['time'].shape[0] / (wtf['time'].iloc[-1] - wtf['time'].iloc[0]),\n",
    "    })\n",
    "    signal = load_data(data_path=temp.name, fs=0, start_sig=0, end_sig=-1, channel=\"Pleth\", use_tk=True, print_flag=True)\n",
    "''';"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
